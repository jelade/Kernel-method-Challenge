{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kernel Method Challenge ","metadata":{}},{"cell_type":"markdown","source":"### Student Name: Lionel Nanguep Komen and Abduljaleel Adejumo\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Kernel classes","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport scipy.sparse as sparse\nfrom tqdm import tqdm\nimport pickle\nimport pandas as pd\n\nclass Kernel():\n   \n    def __init__(self):\n        pass\n\n    def similarity(self, x, y):\n        \n        return -1\n\n    def gram(self, X1, X2=None):\n        \"\"\" Compute the gram matrix of a data vector X where the (i,j) entry is defined as <Xi,Xj>\\\\\n        X1: data vector (n_samples_1 x n_features)\n        X2: data vector (n_samples_2 x n_features), if None compute the gram matrix for (X1,X1)\n        \"\"\"\n        if X2 is None: \n            X2=X1\n        n_samples_1 = X1.shape[0]\n        n_samples_2 = X2.shape[0]\n        G = np.zeros((n_samples_1, n_samples_2))\n        for ii in tqdm(range(n_samples_1)):\n            for jj in range(n_samples_2):\n                G[ii,jj] = self.similarity(X1[ii], X2[jj])\n        return G\n\n\nclass SumKernel(Kernel):\n\n    def __init__(self, kernels, weights=None):\n        \"\"\" kernels: list of kernels \"\"\"\n        self.kernels = kernels\n        self.weights = weights\n        if self.weights is None:\n            self.weights = [1.0 for _ in kernels]\n        super().__init__()\n\n    def similarity(self, x, y):\n        \"\"\" x, y: string \"\"\"\n        s = self.kernels[0].similarity(x,y) * self.weights[0]\n        for ii, kernel in enumerate(self.kernels[1:]):\n            s += kernel.similarity(x,y) * self.weights[ii]\n        return s\n\n    def gram(self, X1, X2=None):\n        \"\"\" Compute the sum of the gram matrices of all kernels\\\\\n        X1: array of string (n_samples_1,)\n        X2: array of string (n_samples_2,), if None compute the gram matrix for (X1,X1)\n        \"\"\"\n        G = self.kernels[0].gram(X1,X2) * self.weights[0]\n        for ii, kernel in tqdm(enumerate(self.kernels[1:])):\n            G += kernel.gram(X1,X2) * self.weights[ii]\n        return G\n\n\nclass LinearKernel(Kernel):\n\n    def __init__(self):\n        super().__init__()\n\n    def similarity(self, x, y):\n        \"\"\" linear kernel : k(x,y) = <x,y> \\\\\n        x, y: array (n_features,)\n        \"\"\"\n        return np.dot(x,y)\n\n\nclass GaussianKernel(Kernel):\n\n    def __init__(self, sigma,normalize=True):\n        super().__init__()\n        self.sigma = sigma\n        self.normalize = normalize\n\n    def similarity(self, x, y):\n        \"\"\" gaussian kernel : k(x,y) = 1/ sqrt(2 pi sigma2)^n * exp( - ||x-y||^2 / 2 sigma^2 )\\\\\n        x, y: array (n_features,)\n        \"\"\"\n\n        if self.normalize:\n            norm_fact = (np.sqrt(2 * np.pi) * self.sigma) ** len(x)\n            return np.exp(-np.linalg.norm(x-y)**2 / (2 * self.sigma**2)) / norm_fact\n        else:\n            return np.exp(-np.linalg.norm(x-y)**2 / (2 * self.sigma**2))\n\n\nclass PolynomialKernel(Kernel):\n\n    def __init__(self, gamma=1, coef0=1, degree=3):\n        super().__init__()\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.degree = degree\n\n    def similarity(self, x, y):\n        \"\"\" polynomial kernel : k(x,y) = (gamma <x,y> + r)^d \\\\\n        x, y: array (n_features,)\n        \"\"\"\n        return (self.gamma * np.dot(x,y) + self.coef0)**self.degree\n\n\nclass SpectrumKernel(Kernel):\n\n    def __init__(self, k):\n        super().__init__()\n        self.k = k\n\n    def similarity(self, x, y):\n        \"\"\" Spectrum kernel \\\\\n        x, y: string\n        \"\"\"\n        substr_x, counts_x = np.unique([x[i:i+self.k] for i in range(len(x)-self.k+1)], return_counts=True)\n        return np.sum(np.char.count(y, substr_x)*counts_x)\n\n\nclass MismatchKernel(Kernel):\n\n    def __init__(self, k, m, neighbours, kmer_set, normalize=False):\n        super().__init__()\n        self.k = k\n        self.m = m\n        self.kmer_set = kmer_set #kmer_set and neighbours have to be pre-computed (to save computational time when running multiple experiments)\n        self.neighbours = neighbours\n        self.normalize = normalize\n\n    def neighbour_embed_kmer(self, x):\n        \"\"\"\n        Embed kmer with neighbours.\n        x: str\n        \"\"\"\n        kmer_x = [x[j:j + self.k] for j in range(len(x) - self.k + 1)]\n        x_emb = {}\n        for kmer in kmer_x:\n            neigh_kmer = self.neighbours[kmer]\n            for neigh in neigh_kmer:\n                idx_neigh = self.kmer_set[neigh]\n                if idx_neigh in x_emb:\n                    x_emb[idx_neigh] += 1\n                else:\n                    x_emb[idx_neigh] = 1\n        return x_emb\n        \n\n    def neighbour_embed_data(self, X):\n        \"\"\"\n        Embed data with neighbours.\n        X: array of string\n        \"\"\"\n        X_emb = []\n        for i in range(len(X)):\n            x = X[i]\n            x_emb = self.neighbour_embed_kmer(x)\n            X_emb.append(x_emb)\n        return X_emb\n    \n    def to_sparse(self, X_emb):\n        \"\"\"\n        Embed data to sparse matrix.\n        X_emb: list of dict.\n        \"\"\"\n        data, row, col = [], [], []\n        for i in range(len(X_emb)):\n            x = X_emb[i]\n            data += list(x.values())\n            row += list(x.keys())\n            col += [i for j in range(len(x))]\n        X_sm = sparse.coo_matrix((data, (row, col)))\n        return X_sm\n\n\n\n    def similarity(self, x, y):\n        \"\"\" Mismatch kernel \\\\\n        x, y: string\n        \"\"\"\n        x_emb = self.neighbour_embed_kmer(x)\n        y_emb = self.neighbour_embed_kmer(y)\n        sp = 0\n        for idx_neigh in x_emb:\n            if idx_neigh in y_emb:\n                sp += x_emb[idx_neigh] * y_emb[idx_neigh]\n        if self.normalize:\n            sp /= np.sqrt(np.sum(np.array(list(x_emb.values()))**2))\n            sp /= np.sqrt(np.sum(np.array(list(y_emb.values()))**2))\n        return sp\n\n    def gram(self, X1, X2=None):\n        \"\"\" Compute the gram matrix of a data vector X where the (i,j) entry is defined as <Xi,Xj>\\\\\n        X1: array of string (n_samples_1,)\n        X2: array of string (n_samples_2,), if None compute the gram matrix for (X1,X1)\n        \"\"\"\n        \n        X1_emb = self.neighbour_embed_data(X1)\n        X1_sm = self.to_sparse(X1_emb)\n        \n        if X2 is None:\n            X2 = X1\n        X2_emb = self.neighbour_embed_data(X2)\n        X2_sm = self.to_sparse(X2_emb)\n\n        # Reshape matrices if the sizes are different\n        nadd_row = abs(X1_sm.shape[0] - X2_sm.shape[0])\n        if X1_sm.shape[0] > X2_sm.shape[0]:\n            add_row = sparse.coo_matrix(([0], ([nadd_row-1], [X2_sm.shape[1]-1])))\n            X2_sm = sparse.vstack((X2_sm, add_row))\n        elif X1_sm.shape[0] < X2_sm.shape[0]:\n            add_row = sparse.coo_matrix(([0], ([nadd_row - 1], [X1_sm.shape[1] - 1])))\n            X1_sm = sparse.vstack((X1_sm, add_row))\n\n        G = (X1_sm.T * X2_sm).todense().astype('float')\n        \n        if self.normalize:\n            G /= np.array(np.sqrt(X1_sm.power(2).sum(0)))[0,:,None]\n            G /= np.array(np.sqrt(X2_sm.power(2).sum(0)))[0,None,:]\n            \n        return G\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:09:57.425256Z","iopub.execute_input":"2023-07-30T22:09:57.425746Z","iopub.status.idle":"2023-07-30T22:09:57.537315Z","shell.execute_reply.started":"2023-07-30T22:09:57.425692Z","shell.execute_reply":"2023-07-30T22:09:57.536260Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Kmer","metadata":{}},{"cell_type":"code","source":"def create_kmer_set(X, k, kmer_set={}):\n    \"\"\"\n    Return a set of all kmers appearing in the dataset.\n    \"\"\"\n    len_seq = len(X[0])\n    idx = len(kmer_set)\n    for i in range(len(X)):\n        x = X[i]\n        kmer_x = [x[i:i + k] for i in range(len_seq - k + 1)]\n        for kmer in kmer_x:\n            if kmer not in kmer_set:\n                kmer_set[kmer] = idx\n                idx += 1\n    return kmer_set\n\n\ndef m_neighbours(kmer, m, recurs=0):\n    \"\"\"\n    Return a list of neighbours kmers (up to m mismatches).\n    \"\"\"\n    if m == 0:\n        return [kmer]\n\n    letters = ['G', 'T', 'A', 'C']\n    k = len(kmer)\n    neighbours = m_neighbours(kmer, m - 1, recurs + 1)\n\n    for j in range(len(neighbours)):\n        neighbour = neighbours[j]\n        for i in range(recurs, k - m + 1):\n            for l in letters:\n                neighbours.append(neighbour[:i] + l + neighbour[i + 1:])\n    return list(set(neighbours))\n\n\ndef get_neighbours(kmer_set, m):\n    \"\"\"\n    Find the neighbours given a set of kmers.\n    \"\"\"\n    kmers_list = list(kmer_set.keys())\n    kmers = np.array(list(map(list, kmers_list)))\n    num_kmers, kmax = kmers.shape\n    neighbours = {}\n    for i in range(num_kmers):\n        neighbours[kmers_list[i]] = []\n\n    for i in tqdm(range(num_kmers)):\n        kmer = kmers_list[i]\n        kmer_neighbours = m_neighbours(kmer, m)\n        for neighbour in kmer_neighbours:\n            if neighbour in kmer_set:\n                neighbours[kmer].append(neighbour)\n    return neighbours\n\n\ndef load_neighbors(dataset, k, m):\n    \"\"\"\n    dataset: 0, 1 or 2\\\\\n    k: len of the kmers\n    m: number of possible mismatches\n    \"\"\"\n    file_name = 'neighbours_'+str(dataset)+'_'+str(k)+'_'+str(m)+'.p'\n    # Load\n    neighbours, kmer_set = pickle.load(open(file_name, 'rb'))\n    print('Neighbors correctly loaded!')\n    return neighbours, kmer_set\n\n\ndef load_or_compute_neighbors(dataset,k,m):\n    \"\"\"\n    dataset: 0, 1 or 2\\\\\n    k: len of the kmers\n    m: number of possible mismatches\n    \"\"\"\n    \n    try:\n        #Load the neighbors\n        neighbours, kmer_set = load_neighbors(dataset, k, m)\n    except:\n        print('No file found, creating kmers neighbors')\n        #Compute the neighbors\n        file_name = 'neighbours_'+str(dataset)+'_'+str(k)+'_'+str(m)+'.p'\n        if dataset==0:\n            X0_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr0.csv\", sep=\",\", index_col=0).values\n            X0_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte0.csv\", sep=\",\", index_col=0).values\n            kmer_set = create_kmer_set(X0_train[:,0], k, kmer_set={})\n            kmer_set = create_kmer_set(X0_test[:,0], k, kmer_set)\n            neighbours = get_neighbours(kmer_set, m)\n            pickle.dump([neighbours, kmer_set], open(file_name, 'wb'))\n        elif dataset==1:\n            X1_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr1.csv\", sep=\",\", index_col=0).values\n            X1_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte1.csv\", sep=\",\", index_col=0).values\n            kmer_set = create_kmer_set(X1_train[:,0], k, kmer_set={})\n            kmer_set = create_kmer_set(X1_test[:,0], k, kmer_set)\n            neighbours = get_neighbours(kmer_set, m)\n            pickle.dump([neighbours, kmer_set], open(file_name, 'wb'))\n        elif dataset==2:\n            X2_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr2.csv\", sep=\",\", index_col=0).values\n            X2_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte2.csv\", sep=\",\", index_col=0).values\n            kmer_set = create_kmer_set(X2_train[:,0], k, kmer_set={})\n            kmer_set = create_kmer_set(X2_test[:,0], k, kmer_set)\n            neighbours = get_neighbours(kmer_set, m)\n            pickle.dump([neighbours, kmer_set], open(file_name, 'wb'))\n            \n    return neighbours, kmer_set","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:02.553555Z","iopub.execute_input":"2023-07-30T22:22:02.554022Z","iopub.status.idle":"2023-07-30T22:22:02.583646Z","shell.execute_reply.started":"2023-07-30T22:22:02.553988Z","shell.execute_reply":"2023-07-30T22:22:02.581981Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install cvxopt","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:10:00.681353Z","iopub.execute_input":"2023-07-30T22:10:00.681891Z","iopub.status.idle":"2023-07-30T22:10:16.603141Z","shell.execute_reply.started":"2023-07-30T22:10:00.681849Z","shell.execute_reply":"2023-07-30T22:10:16.601257Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting cvxopt\n  Downloading cvxopt-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cvxopt\nSuccessfully installed cvxopt-1.3.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Classifiers","metadata":{}},{"cell_type":"code","source":"# Ridge Regression (RR)\ndef solveRR(y, X, lam):\n    n, p = X.shape\n    assert (len(y) == n)\n    \n    A = X.T.dot(X)\n    # Adjust diagonal due to Ridge\n    # A[np.diag_indices_from(A)] += lam * n\n    A += n * lam * np.eye(p)\n    b = X.T.dot(y)\n    # Hint:\n    beta = np.linalg.solve(A, b)\n    # Finds solution to the linear system Ax = b\n    return (beta)\n\n\n# Weighted Ridge Regression (WRR)\ndef solveWRR(y, X, w, lam):\n    n, p = X.shape\n    assert (len(y) == len(w) == n)\n\n    w_sqrt = np.sqrt(w)\n    \n    y1 = w_sqrt * y\n    X1 = X * w_sqrt[:, None] \n    # Or X1 = np.diag(w_sqrt) @ X # (Less efficient)\n\n    # Hint:\n    # Find y1 and X1 such that:\n    beta = solveRR(y1, X1, lam)\n    return (beta)\n\n\n\n# Logistic Ridge Regression (LRR) with gradient descent (GD)\ndef solveLRR_gradient(y, X, lam, h=0.01, max_iter=500, eps=1e-12):\n    '''\n    lam: Regularization parameter\n    max_iter: Max number of iterations of gradient descent\n    eps: Tolerance for stopping criteria \n    '''\n    n, p = X.shape\n    assert (len(y) == n)\n    \n    beta_old = np.zeros(p)\n    \n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n            \n    for i in range(max_iter):\n        # yi beta^T xi\n        f = (X * y[:, None]).dot(beta_old)\n        gradient = - 1 / n * (y * sigmoid(-f)).dot(X)\n        gradient += 2 * lam * beta_old\n        \n        # Step\n        beta_new = beta_old - h * gradient\n        \n        if np.sum((beta_new-beta_old)**2) < eps:\n            break\n        beta_old = beta_new\n    #         \n    return (beta_new)\n\n\n# Logistic Ridge Regression with Newton-Raphson\ndef solveLRR_newton(y, X, lam, max_iter=500, eps=1e-3):\n    n, p = X.shape\n    assert (len(y) == n)\n    \n    # Parameters\n    max_iter = 500\n    eps = 1e-3\n    sigmoid = lambda a: 1/(1 + np.exp(-a))\n    \n    # Initialize\n    beta = np.zeros(p)\n            \n    # Hint: Use IRLS\n    for i in range(max_iter):\n        beta_old = beta\n        f = X.dot(beta_old)\n        w = sigmoid(f) * sigmoid(-f)\n        z = f + y / sigmoid(y*f)\n        beta = solveWRR(z, X, w, 2*lam)\n        # Break condition (achieved convergence)\n        if np.sum((beta-beta_old)**2) < eps:\n            break\n    return (beta)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:22.392527Z","iopub.execute_input":"2023-07-30T22:22:22.392956Z","iopub.status.idle":"2023-07-30T22:22:22.413007Z","shell.execute_reply.started":"2023-07-30T22:22:22.392923Z","shell.execute_reply":"2023-07-30T22:22:22.411622Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KernelRidgeClassifier(KernelRidgeRegression):\n    '''\n    Kernel Ridge Classification\n    '''\n    def predict(self, X):\n        return np.sign(super().predict(X))\n    \n\nclass KernelRidgeClassifier(KernelRidgeRegression):\n    '''\n    Kernel Ridge Classification\n    '''\n    def predict(self, X):\n        return np.sign(super().predict(X))","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:23.823459Z","iopub.execute_input":"2023-07-30T22:22:23.823965Z","iopub.status.idle":"2023-07-30T22:22:23.831790Z","shell.execute_reply.started":"2023-07-30T22:22:23.823921Z","shell.execute_reply":"2023-07-30T22:22:23.830153Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import cvxopt\nfrom cvxopt import matrix\n\nclass SVM():\n    \"\"\"\n    SVM implementation\n    \n    Usage:\n        svm = SVM(kernel='linear', C=1)\n        svm.fit(X_train, y_train)\n        svm.predict(X_test)\n    \"\"\"\n\n    def __init__(self, kernel, C=1.0, tol_support_vectors=1e-4):\n        \"\"\"\n        kernel: Which kernel to use\n        C: float > 0, default=1.0, regularization parameter\n        tol_support_vectors: Threshold for alpha value to consider vectors as support vectors\n        \"\"\"\n        self.kernel = kernel\n        self.C = C\n        self.tol_support_vectors = tol_support_vectors\n\n    def fit(self, X, y):\n\n        self.X_train = X\n        n_samples = X.shape[0]\n        print(\"Computing the kernel...\")\n        self.X_train_gram = self.kernel.gram(X)\n        print(\"Done!\")\n\n        #Define the optimization problem to solve\n\n        P = self.X_train_gram\n        q = -y.astype('float')\n        G = np.block([[np.diag(np.squeeze(y).astype('float'))],[-np.diag(np.squeeze(y).astype('float'))]])\n        h = np.concatenate((self.C*np.ones(n_samples),np.zeros(n_samples)))\n\n        #Solve the problem\n        #With cvxopt\n\n        P=matrix(P)\n        q=matrix(q)\n        G=matrix(G)\n        h=matrix(h)\n        solver = cvxopt.solvers.qp(P=P,q=q,G=G,h=h)\n        x = solver['x']\n        self.alphas = np.squeeze(np.array(x))\n\n        #Retrieve the support vectors\n        self.support_vectors_indices = np.squeeze(np.abs(np.array(x))) > self.tol_support_vectors\n        self.alphas = self.alphas[self.support_vectors_indices]\n        self.support_vectors = self.X_train[self.support_vectors_indices]\n\n        print(len(self.support_vectors), \"support vectors out of\",len(self.X_train), \"training samples\")\n\n        return self.alphas\n\n\n    def predict(self, X):\n        \"\"\"\n        X: array (n_samples, n_features)\\\\\n        Return: float array (n_samples,)\n        \"\"\"\n        K = self.kernel.gram(X, self.support_vectors)\n        y = np.dot(K, self.alphas)\n        return y\n\n    def predict_classes(self, X, threshold=0):\n        \"\"\"\n        X: array (n_samples, n_features)\\\\\n        Return: 0 and 1 array (n_samples,)\n        \"\"\"\n        K = self.kernel.gram(X, self.support_vectors)\n        y = np.dot(K, self.alphas)\n        return np.where(y > threshold, 1, -1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:24.584501Z","iopub.execute_input":"2023-07-30T22:22:24.585470Z","iopub.status.idle":"2023-07-30T22:22:24.618091Z","shell.execute_reply.started":"2023-07-30T22:22:24.585416Z","shell.execute_reply":"2023-07-30T22:22:24.616459Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:26.103413Z","iopub.execute_input":"2023-07-30T22:22:26.103886Z","iopub.status.idle":"2023-07-30T22:22:26.110639Z","shell.execute_reply.started":"2023-07-30T22:22:26.103852Z","shell.execute_reply":"2023-07-30T22:22:26.108854Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nIndividual mismatch try\n\"\"\"\n\n# Imports\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\n\n\n# Read csv files\n\n# shape (2000,1): string\nX0_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr0.csv\", sep=\",\", index_col=0).values\nX1_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr1.csv\", sep=\",\", index_col=0).values\nX2_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr2.csv\", sep=\",\", index_col=0).values\n\n# shape (2000,100): float\nX0_mat100_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr0_mat100.csv\", sep=\" \", header=None).values\nX1_mat100_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr1_mat100.csv\", sep=\" \", header=None).values\nX2_mat100_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr2_mat100.csv\", sep=\" \", header=None).values\n\n# shape (2000,1): string\nX0_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte0.csv\", sep=\",\", index_col=0).values\nX1_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte1.csv\", sep=\",\", index_col=0).values\nX2_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte2.csv\", sep=\",\", index_col=0).values\n\n# shape (2000,100): float\nX0_mat100_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte0_mat100.csv\", sep=\" \", header=None).values\nX1_mat100_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte1_mat100.csv\", sep=\" \", header=None).values\nX2_mat100_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte2_mat100.csv\", sep=\" \", header=None).values\n\n# shape (2000,1): 0 or 1\nY0_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr0.csv\", sep=\",\", index_col=0).values\nY1_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr1.csv\", sep=\",\", index_col=0).values\nY2_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr2.csv\", sep=\",\", index_col=0).values\n\n\n\n\n\n## Preprocessing\n\nfraction_of_data = 0.2 #Put a small value for faster tests\nsplit_ratio = 0.8 #Ratio of data in train set\nshuffle = True #Shuffle the data\nrescale_y = True #Rescale labels to -1 and 1\n\n#Creating validation set and prediction set\nY_train_full = np.where(Y2_train == 0, -1, 1)\nX_train, X_val,Y_train, Y_val = train_test_split(X2_train, Y_train_full, test_size=0.2, random_state=42)\nX_mat_train, X_mat_val,Y_train, Y_val = train_test_split(X1_mat100_train, Y_train_full, test_size=0.2, random_state=42)\n\n\n\n\ntest_our_svm = False\ntest_spectrum = False\ntest_mismatch = True\n\nif test_our_svm:\n\n    ## Test our SVM implementation\n\n    #Parameters\n    kernel = 'poly' # 'linear' 'rbf' or 'poly'\n    C = 1.0\n    gamma = 1/(X_mat_train.shape[1] * X_mat_train.var())\n    coef0 = 1.0\n    degree = 3\n\n    print(\"Kernel:\", kernel)\n    print(\"C:\", C)\n    if kernel != 'linear':\n        print(\"Gamma:\", gamma)\n    if kernel == 'poly':\n        print(\"Coef0:\", coef0)\n        print(\"Degree:\", degree)\n    print()\n\n    #Our SVM\n    print(\"Applying our SVM...\")\n    if kernel=='linear':\n        our_svm = SVM(kernel=LinearKernel(),C=C)\n    elif kernel=='rbf':\n        our_svm = SVM(kernel=GaussianKernel(sigma=np.sqrt(0.5/gamma),normalize=False),C=C)\n    elif kernel=='poly':\n        our_svm = SVM(kernel=PolynomialKernel(gamma=gamma,coef0=coef0,degree=degree),C=C)\n    our_svm.fit(X_mat_train, Y_train)\n    our_svm_classes_train = our_svm.predict_classes(X_mat_train)\n    our_svm_classes_val = our_svm.predict_classes(X_mat_val)\n\n    print(\"Accuracy on train (our SVM):\", np.sum(np.squeeze(our_svm_classes_train)==np.squeeze(Y_train))/len(Y_train))    \n    print(\"Accuracy on val (our SVM):\", np.sum(np.squeeze(our_svm_classes_val)==np.squeeze(Y_val))/len(Y_val))\n\n\n\nif test_mismatch:\n\n    def create_kmer_set(X, k, kmer_set={}):\n        \"\"\"\n        Return a set of all kmers appearing in the dataset.\n        \"\"\"\n        len_seq = len(X[0])\n        idx = len(kmer_set)\n        for i in range(len(X)):\n            x = X[i]\n            kmer_x = [x[i:i + k] for i in range(len_seq - k + 1)]\n            for kmer in kmer_x:\n                if kmer not in kmer_set:\n                    kmer_set[kmer] = idx\n                    idx += 1\n        return kmer_set\n\n\n    def m_neighbours(kmer, m, recurs=0):\n        \"\"\"\n        Return a list of neighbours kmers (up to m mismatches).\n        \"\"\"\n        if m == 0:\n            return [kmer]\n\n        letters = ['G', 'T', 'A', 'C']\n        k = len(kmer)\n        neighbours = m_neighbours(kmer, m - 1, recurs + 1)\n\n        for j in range(len(neighbours)):\n            neighbour = neighbours[j]\n            for i in range(recurs, k - m + 1):\n                for l in letters:\n                    neighbours.append(neighbour[:i] + l + neighbour[i + 1:])\n        return list(set(neighbours))\n\n\n    def get_neighbours(kmer_set, m):\n        \"\"\"\n        Find the neighbours given a set of kmers.\n        \"\"\"\n        kmers_list = list(kmer_set.keys())\n        kmers = np.array(list(map(list, kmers_list)))\n        num_kmers, kmax = kmers.shape\n        neighbours = {}\n        for i in range(num_kmers):\n            neighbours[kmers_list[i]] = []\n\n        for i in tqdm(range(num_kmers)):\n            kmer = kmers_list[i]\n            kmer_neighbours = m_neighbours(kmer, m)\n            for neighbour in kmer_neighbours:\n                if neighbour in kmer_set:\n                    neighbours[kmer].append(neighbour)\n        return neighbours\n    \n    k = 10\n    m = 1\n\n    try:\n        # Load\n        neighbours, kmer_set = pickle.load(open('neighbours_22'+str(k)+'_'+str(m)+'.p', 'rb'))\n        print('Neighbors correctly loaded')\n    except:\n        print('No file found, creating kmers neighbors')\n        kmer_set = create_kmer_set(X2_train[:,0], k)\n        kmer_set = create_kmer_set(X2_test[:,0], k, kmer_set)\n        neighbours = get_neighbours(kmer_set, m)\n        \n        # Save neighbours and kmer set\n        pickle.dump([neighbours, kmer_set], open('neighbours_22'+str(k)+'_'+str(m)+'.p', 'wb'))\n\n    print('Doing SVM')\n    C = 1\n    svm = SVM(kernel=MismatchKernel(k=k, m=m, neighbours=neighbours, kmer_set=kmer_set), C=C)\n    \n    # X0_train, X0_val = X0_train[:1600], X0_train[1600:]\n\n    # Y0_train = np.where(Y0_train == 0, -1, 1)\n    # Y0_train, Y0_val = Y0_train[:1600], Y0_train[1600:]\n\n    svm.fit(X_train[:,0], Y_train)\n\n    pred_train = svm.predict_classes(X_train[:,0])\n    # pred = np.where(pred == -1, 0, 1) \n    print( np.sum(np.squeeze(pred_train)==np.squeeze(Y_train)) / len(Y_train) )\n    pred_val = svm.predict_classes(X_val[:,0])\n    # pred = np.where(pred == -1, 0, 1)\n    \n    print( np.sum(np.squeeze(pred_val)==np.squeeze(Y_val)) / len(Y_val) )\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:42.122745Z","iopub.execute_input":"2023-07-30T22:22:42.123179Z","iopub.status.idle":"2023-07-30T22:23:04.578354Z","shell.execute_reply.started":"2023-07-30T22:22:42.123140Z","shell.execute_reply":"2023-07-30T22:23:04.576822Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Neighbors correctly loaded\nDoing SVM\nComputing the kernel...\nDone!\n     pcost       dcost       gap    pres   dres\n 0: -6.6990e-03 -1.6108e+03  6e+03  1e+00  4e-15\n 1:  1.2980e+00 -4.3057e+02  5e+02  2e-02  4e-15\n 2:  9.1395e-01 -6.3230e+01  7e+01  3e-03  4e-15\n 3: -2.3402e-01 -7.0133e+00  7e+00  2e-04  3e-15\n 4: -6.1989e-01 -9.1297e-01  3e-01  2e-06  2e-15\n 5: -6.4188e-01 -6.8678e-01  4e-02  3e-07  9e-16\n 6: -6.4434e-01 -6.5172e-01  7e-03  3e-08  8e-16\n 7: -6.4482e-01 -6.4587e-01  1e-03  4e-09  8e-16\n 8: -6.4491e-01 -6.4499e-01  9e-05  3e-10  1e-15\n 9: -6.4491e-01 -6.4492e-01  2e-06  3e-12  1e-15\n10: -6.4492e-01 -6.4492e-01  7e-08  6e-14  1e-15\nOptimal solution found.\n1377 support vectors out of 1600 training samples\n1.0\n0.7625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Prediction by replacing the data\n\npred_train1 = svm.predict_classes(X0_test[:,0])\npred_train2 = svm.predict_classes(X1_test[:,0])\npred_train3 = svm.predict_classes(X2_test[:,0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1= np.arange(3000).reshape(-1, 1)\nsample = pd.DataFrame(data=X1, columns=['Id'])\nsample.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_ = np.concatenate([pred_train1, pred_train2,pred_train3], axis=1).T.flatten()\n\nprint(len(y_pred_))\ny_pred_\n\ny_pred_f = np.where(y_pred_==-1.0, 0, 1)\ny_pred_f\n\nsample['Bound'] = y_pred_f\nsample.tail()\nsample.to_csv('subkernel3.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final model","metadata":{}},{"cell_type":"code","source":"\n##### LOAD DATA #####\n\n# shape (2000,1): string\nX0_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr0.csv\", sep=\",\", index_col=0).values\nX1_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr1.csv\", sep=\",\", index_col=0).values\nX2_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xtr2.csv\", sep=\",\", index_col=0).values\n\n\n\n# shape (2000,1): string\nX0_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte0.csv\", sep=\",\", index_col=0).values\nX1_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte1.csv\", sep=\",\", index_col=0).values\nX2_test = pd.read_csv(\"/kaggle/input/bhnkkjkj/Xte2.csv\", sep=\",\", index_col=0).values\n\n\n\n# shape (2000,1): 0 or 1\nY0_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr0.csv\", sep=\",\", index_col=0).values\nY1_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr1.csv\", sep=\",\", index_col=0).values\nY2_train = pd.read_csv(\"/kaggle/input/bhnkkjkj/Ytr2.csv\", sep=\",\", index_col=0).values\n\n\n##### PREPROCESS DATA #####\n\n#Rescaling labels\nY0_train = np.where(Y0_train == 0, -1, 1)\nY1_train = np.where(Y1_train == 0, -1, 1)\nY2_train = np.where(Y2_train == 0, -1, 1)\n\n##### PARAMETERS #####\n\nC = 1\nweights = [1.0,1.0,1.0,1.0,1.0,1.0] #List of weights for sum of mismatch kernels\nlist_k = [5,8,9,10,12,13] #List of parameters k for sum of mismatch kernels\nlist_m = [1,1,1,1,2,2]\n\n#Shuffling\n\n\nshuffling_0 = np.random.permutation(len(X0_train))\nX0_train = X0_train[shuffling_0][:,0]\n\nY0_train = Y0_train[shuffling_0]\n\nshuffling_1 = np.random.permutation(len(X1_train))\nX1_train = X1_train[shuffling_1][:,0]\n\nY1_train = Y1_train[shuffling_1]\n\nshuffling_2 = np.random.permutation(len(X2_train))\nX2_train = X2_train[shuffling_2][:,0]\n\nY2_train = Y2_train[shuffling_2]\n\n\n#Put test matrices into the right format\nX0_test = X0_test[:,0]\nX1_test = X1_test[:,0]\nX2_test = X2_test[:,0]\n\n\n##### APPLY SVM ON DATASET 0 #####\n\nprint(\"Applying SVM on dataset 0...\")\n\n\ndataset_nbr = 0 \nkernels = []\nfor k,m in zip(list_k,list_m):\n    neighbours, kmer_set = load_or_compute_neighbors(dataset_nbr, k, m)\n    kernels.append(MismatchKernel(k=k, m=m, neighbours=neighbours, kmer_set=kmer_set, normalize = True))\nsvm = SVM(kernel=SumKernel(kernels=kernels, weights=weights), C=C)\n\n\nsvm.fit(X0_train, Y0_train)\npred_0 = svm.predict_classes(X0_test)\n\n##### APPLY SVM ON DATASET 1 #####\n\nprint(\"Applying SVM on dataset 1...\")\n\n\ndataset_nbr = 1\nkernels = []\nfor k,m in zip(list_k,list_m):\n    neighbours, kmer_set = load_or_compute_neighbors(dataset_nbr, k, m)\n    kernels.append(MismatchKernel(k=k, m=m, neighbours=neighbours, kmer_set=kmer_set, normalize = True))\nsvm = SVM(kernel=SumKernel(kernels=kernels, weights=weights), C=C)\n\nsvm.fit(X1_train, Y1_train)\npred_1 = svm.predict_classes(X1_test)\n\n\n##### APPLY SVM ON DATASET 2 #####\n\nprint(\"Applying SVM on dataset 2...\")\n\n\ndataset_nbr = 2\nkernels = []\nfor k,m in zip(list_k,list_m):\n    neighbours, kmer_set = load_or_compute_neighbors(dataset_nbr, k, m)\n    kernels.append(MismatchKernel(k=k, m=m, neighbours=neighbours, kmer_set=kmer_set, normalize = True))\nsvm = SVM(kernel=SumKernel(kernels=kernels, weights=weights), C=C)\n\nsvm.fit(X2_train, Y2_train)\npred_2 = svm.predict_classes(X2_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T22:22:34.381085Z","iopub.status.idle":"2023-07-30T22:22:34.381493Z","shell.execute_reply.started":"2023-07-30T22:22:34.381303Z","shell.execute_reply":"2023-07-30T22:22:34.381322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### CREATE SUBMISSION FILE #####\n\npred = np.concatenate([pred_0.squeeze(),pred_1.squeeze(),pred_2.squeeze()])\npred = np.where(pred == -1, 0, 1)\npred_df = pd.DataFrame()\nprint(pred.shape)\nprint(pred)\npred_df['Bound'] = pred\npred_df.index.name = 'Id'\npred_df.to_csv('prediction.csv', sep=',', header=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T00:23:09.970188Z","iopub.status.idle":"2023-07-29T00:23:09.970773Z","shell.execute_reply.started":"2023-07-29T00:23:09.970563Z","shell.execute_reply":"2023-07-29T00:23:09.970584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_train1 = svm.predict_classes(X0_test[:,0])\npred_train1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}